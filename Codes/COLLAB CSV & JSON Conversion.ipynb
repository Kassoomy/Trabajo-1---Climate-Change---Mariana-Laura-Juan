{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNuiI0MS8gw9mGMFWH1PMF3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Libraries SetUp"],"metadata":{"id":"nVcwKjK3ilQ3"}},{"cell_type":"markdown","source":["## General"],"metadata":{"id":"ztcJOnhWivac"}},{"cell_type":"code","source":["#instalar java y spark\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q https://downloads.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n","!tar xf spark-3.3.0-bin-hadoop3.tgz\n","!pip install -q findspark\n","!pip install boto3\n","!pip install s3fs"],"metadata":{"id":"KXlvyrAm-wFT","executionInfo":{"status":"ok","timestamp":1663120749511,"user_tz":300,"elapsed":35673,"user":{"displayName":"Juan Gallego TheRiffbuddy","userId":"09910600665367632445"}},"colab":{"base_uri":"https://localhost:8080/","height":867},"outputId":"f109b363-7d4d-408f-977a-e4ba054bd993"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.24.72)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.6.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.0.1)\n","Requirement already satisfied: botocore<1.28.0,>=1.27.72 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.27.72)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.72->boto3) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.72->boto3) (1.26.12)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.72->boto3) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting s3fs\n","  Downloading s3fs-2022.8.2-py3-none-any.whl (27 kB)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from s3fs) (3.8.1)\n","Requirement already satisfied: fsspec==2022.8.2 in /usr/local/lib/python3.7/dist-packages (from s3fs) (2022.8.2)\n","Collecting aiobotocore~=2.4.0\n","  Downloading aiobotocore-2.4.0-py3-none-any.whl (65 kB)\n","\u001b[K     |████████████████████████████████| 65 kB 2.2 MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.7/dist-packages (from aiobotocore~=2.4.0->s3fs) (1.14.1)\n","Collecting botocore<1.27.60,>=1.27.59\n","  Downloading botocore-1.27.59-py3-none-any.whl (9.1 MB)\n","\u001b[K     |████████████████████████████████| 9.1 MB 27.5 MB/s \n","\u001b[?25hCollecting aioitertools>=0.5.1\n","  Downloading aioitertools-0.10.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.1.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.13.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (22.1.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.1.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.8.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.0->s3fs) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.0->s3fs) (1.26.12)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.0->s3fs) (1.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.0->s3fs) (1.15.0)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.10)\n","Installing collected packages: botocore, aioitertools, aiobotocore, s3fs\n","  Attempting uninstall: botocore\n","    Found existing installation: botocore 1.27.72\n","    Uninstalling botocore-1.27.72:\n","      Successfully uninstalled botocore-1.27.72\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","boto3 1.24.72 requires botocore<1.28.0,>=1.27.72, but you have botocore 1.27.59 which is incompatible.\u001b[0m\n","Successfully installed aiobotocore-2.4.0 aioitertools-0.10.0 botocore-1.27.59 s3fs-2022.8.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["botocore"]}}},"metadata":{}}]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\""],"metadata":{"id":"nFzTsM6f-ws7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import findspark\n","findspark.init()"],"metadata":{"id":"_sH9aP7N-yz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gjzuk-z_YqPw"},"outputs":[],"source":["import pandas as pd \n","import json\n","from pyspark.sql.functions import explode, col\n","import boto3"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4YuRjoW6ynu","executionInfo":{"status":"ok","timestamp":1663029326163,"user_tz":300,"elapsed":23384,"user":{"displayName":"Juan Gallego TheRiffbuddy","userId":"09910600665367632445"}},"outputId":"09dffb4a-9841-4144-aeb9-7805b175d0c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","source":["## PySpark Conection with AWS"],"metadata":{"id":"vncRCOolKwzY"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","# en el cluster EMR no hay necesidad de crear el objeto spark ni sc, ya viene con AWS EMR / Notebooks\n","\n","# spark local:\n","# spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","\n","# spark conectado con Amazon AWS:\n","\n","spark = SparkSession.builder \\\n","    .appName(\"data_processing\")\\\n","    .master(\"local[*]\")\\\n","    .config(\"spark.driver.memory\",\"16G\")\\\n","    .config(\"spark.driver.maxResultSize\", \"0\") \\\n","    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n","    .config(\"spark.jars.packages\", \"com.amazonaws:aws-java-sdk:1.11.1034,org.apache.hadoop:hadoop-aws:3.3.4\")\\\n","    .config('fs.s3a.access.key', 'ASIA2BEUH3HWEEWRXBJK') \\\n","    .config('fs.s3a.secret.key', 'iI88t6MbfWm/6Wd85TfgCeVS426o53rCDzqOH3dc') \\\n","    .config('fs.s3a.session.token', 'FwoGZXIvYXdzECkaDGoXzXXNlcMMf7YgpyLGARyUcFhXN8O86erw5UHkVmUoH0+6NW4JzNq0pu/Cw453rVQGKxm9f0ywDZAAhuYXjJoLcFcdQYFu4bTsnUguBAac6AkKbF350XWZtYFSlF0QsoVfXpsAdkWFKKLyWnL/kZjRpwpyy3uPPhPBSISE2EPbYRm1lxCkTqm5jjnZ/zlFKAA6cxCbW/ZuPglbOhCpmgkHDpJhfI7GbGFaJ2/M4Y0sjEeQOSKmI/98u+9Xp7D2JzPd61gSf5AwCMQ13NkMo6+eYly0+iidnoSZBjItNa4JThnFCxgxNUC+J9zp79a1pGVj0FenLSc8V7DiJN5eCEFmvA/MFfYdRTsq') \\\n","    .config('fs.s3a.path.style.access', 'true') \\\n","    .config('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n","    .config('fs.s3a.endpoint', 's3.amazonaws.com') \\\n","    .getOrCreate()\n","\n","#create spark session object\n","#spark=SparkSession.builder.appName('data_processing').getOrCreate()\n","# en el cluster EMR no hay necesidad de hacer esto, ya viene con AWS EMR / Notebooks\n","sc = spark.sparkContext"],"metadata":{"id":"1JJyIF1Y-93D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["session = boto3.client(\n","    's3',\n","    aws_access_key_id='ASIA2BEUH3HWEEWRXBJK', \n","    aws_secret_access_key='iI88t6MbfWm/6Wd85TfgCeVS426o53rCDzqOH3dc', \n","    aws_session_token='FwoGZXIvYXdzECkaDGoXzXXNlcMMf7YgpyLGARyUcFhXN8O86erw5UHkVmUoH0+6NW4JzNq0pu/Cw453rVQGKxm9f0ywDZAAhuYXjJoLcFcdQYFu4bTsnUguBAac6AkKbF350XWZtYFSlF0QsoVfXpsAdkWFKKLyWnL/kZjRpwpyy3uPPhPBSISE2EPbYRm1lxCkTqm5jjnZ/zlFKAA6cxCbW/ZuPglbOhCpmgkHDpJhfI7GbGFaJ2/M4Y0sjEeQOSKmI/98u+9Xp7D2JzPd61gSf5AwCMQ13NkMo6+eYly0+iidnoSZBjItNa4JThnFCxgxNUC+J9zp79a1pGVj0FenLSc8V7DiJN5eCEFmvA/MFfYdRTsq')"],"metadata":{"id":"WiXeHK0Mh_Sg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# en gdrive:\n","# mainFolder = \"gdrive/MyDrive/STUDY/EAFIT/Ciencia de Datos y Analitica (Maestria)/Semestre 2022-02 (1)/Almacenamiento y Recuperacion de datos/Trabajo 1 Alm & Rec Datos/Data Source/\"\n","# df1 = pd.read_json(mainFolder + 'Datos_SIATA_Vaisala_temperatura.json')\n","\n","# df1 = spark.read.json('s3a://trabajo1-mariana-laura-juan/Transient/Medellin/Datos_SIATA_Vaisala_temperatura.json')\n","\n","df = spark.read.json('s3a://trabajo1-mariana-laura-juan/Transient/Medellin/Datos_SIATA_Vaisala_temperatura.json')\n","df.printSchema()\n","df.show()"],"metadata":{"id":"5DwRRkj0624L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"EXWDFazcbAS5"}},{"cell_type":"markdown","source":["# Medellin JSON to CSV"],"metadata":{"id":"GpegDMSLa4de"}},{"cell_type":"code","source":["df1 = df.select(explode(df.datos))\n","df1.printSchema()\n","print(df1.count())\n","df1.show()"],"metadata":{"id":"4d743jPmk9vi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9MdRw8GEqqqo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2 = df1.rdd.map(lambda row: (row.col[\"fecha\"],row.col[\"datos\"][0][\"variableConsulta\"],row.col[\"datos\"][0][\"calidad\"],row.col[\"datos\"][0][\"valor\"])).toDF([\"fecha\",\"variableConsulta\",\"calidad\",\"valor\"])\n","df2.printSchema()\n","df2.show()\n","print(df2.count())"],"metadata":{"id":"CtpF3uU9ltM4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df2.write.csv('s3a://trabajo1-mariana-laura-juan/Raw/Medellin/Medellin_temperatura')\n","df2.write.option(\"header\",True).csv('s3a://trabajo1-mariana-laura-juan/Raw/Medellin/Medellin_temperatura')"],"metadata":{"id":"DUGFh6PMjctU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Colombia CSV ETL"],"metadata":{"id":"rKQsH7ypbmHy"}},{"cell_type":"code","source":["docsArr = {'Colombia_Annual_Surface_Temperature_Change':[['Year','Temp Change'],'Degree Celsius',0,1], \n","           'Colombia_Climate-related_Disasters_Frequency':[['Year','Drought','Flood','Landslide','Storm','TOTAL','Wildfire'],'Quantity',0,6]}\n","\n","for item in docsArr:\n","  df = spark.read.option(\"header\",True).csv('s3a://trabajo1-mariana-laura-juan/Transient/Colombia/' + item + '.csv')\n","  df1 = df.toPandas()\n","\n","  df1.drop(columns=['ISO2','ISO3','Indicator','Code','Source'], inplace=True)\n","  df2 = df1.drop(columns=['Country','Unit'])\n","  df2 = df2.transpose()\n","  df2 = df2.reset_index(level=0)\n","  df2.columns = docsArr[item][0]\n","  df2 = pd.concat([df1[['Country', 'Unit']],df2])\n","  df2 = df2.reset_index(level=0)\n","  df2 = df2.drop(columns=['index'])\n","  df2[['Country']] = df2[['Country']].fillna('Colombia')\n","  df2[['Unit']] = df2[['Unit']].fillna(docsArr[item][1])\n","  x = docsArr[item][2]\n","  y = docsArr[item][3]\n","  df2.drop(df2.index[x:y], axis=0, inplace=True)\n","  df2.reset_index(drop=True, inplace=True)\n","  # print(df2)\n","  \n","  sparkDF=spark.createDataFrame(df2) \n","  # sparkDF.printSchema()\n","  # sparkDF.show()\n","  sparkDF.coalesce(1).write.option(\"header\",True).csv('s3a://trabajo1-mariana-laura-juan/Raw/Colombia/' + item)\n","\n","\n"],"metadata":{"id":"rbWzQJm5b_aR"},"execution_count":null,"outputs":[]}]}